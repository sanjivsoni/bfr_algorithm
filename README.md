# BFR algorithm
This repo contains Python implimentation of BFR algorithm for clustering of large data sets that cannot be loaded into memory at once. It runs in Single pass over dataset. Intiticely it can be thought as a variant of k-means which canoot be run on entire data set at once. 
It asuumes clusters are normally distributed about a centroid. This code uses a synthetic data set.
This [video](https://www.youtube.com/watch?v=pS38Ofy3Qig) explains the intuition behing the algorithm 
For more background you can refer to these [slides](https://drive.google.com/file/d/1mggdvcENksHRgf_K6tFPSaOo52SpIgVD/view?usp=sharing)

![BFR Galaxies Clustering](/galaxies.png)
## Getting Started

git clone https://github.com/sanjivsoni/bfr_algorithm.git
```
bfr.py <input_file.txt> <n_clusters_int> <output_file.txt> 
```

## Usage

This example clusters data into 5 clusters
```
bfr.py data.txt 5 output.txt
```

### Data Set

This data set was created normally distributing the data. This data set was generated by initialising centroids randomly and data points were created some standard deviations away from centroids. For more robustness, some outliers were included in the dataset. They are indexed as -1.

### Header
data point index, index of cluster where data point belongs to, features / dimensions

### Implimentation Details

In BFR, three sets of data points are maintained and updated regulary
1. Discard Set (DS)
2. Compression Set (CS)
3. Retained Set (RS)

For each cluster in DS and CS, the cluster is summarised by three metrics:
1. N: # points in the cluster.
2. SUM: The sum of coordinates of the points in each dimension.
3. SUMSQ: The sum of squares of coordinates in each dimension.

Here are the steps for BFR:
1. Load 20% of data randomly.
2. Run K-Means with large K (Eg. 5x number of input clusters) on the data in memory using Eucledian distance as similarity measurement.
3. Using K-Means result from step 2, move all the clusters that contain only one point to RS (outliers set)
4. Run K-Means again to cluster the remaining data points with K = # Input Clusters
5. Use K-Means result from Step 4 to generate the DS clusters (i.e discard their points and generate statistics)
At this point, the initialisation of DS has finished  and there are K number of DS clusters and some stats from step 3
6. Run K-Means on the points in the RS with large K (5x # input clusters) to generate CS (clusters with more than one point) and RS (clusters with only one point)
7. Load another 20% of data randomly.
8. For the new points, compare them to each of the DS using [Mahalanobis Distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) and assign them to the nearest DS clusters if the distance is < 2 * sqrt(2)
9. For the new points that are unassigned to DS clusters, using [Mahalanobis Distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) assign the points the nearest CS clusters if the distnace is < 2 * sqrt(2)
10. For the new points that are not assigned to a DS cluster of CS cluster, assign them to RS.
11. Run K-Means on RS with large K (5x # clusters) to generate CS and RS clusters respectively.
12. Merge CS clusters that have [Mahalanobis Distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) < 2 * sqrt(2)
Repeat Steps 7 - 12
If this the final run (after last chunk of data is loaded), merge CS with DS clusters that have Mahalanobis distance < 2 * sqrt(2)


### Output Format
The output file contains two parts:
1. Intermediate results for every round <br>
Round i: <# discard points>, <# clusters in CS>, <# of CS points>, <# of points in RS>
2. Clustering results: <br>
<# data_pointsindex>, <#clustering result> <br>
Note: Cluster of outliers will be represented as -1

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details

## Acknowledgements
Bradley, Fayyad and Reina <br>
[Link to Original Paper ](https://www.aaai.org/Papers/KDD/1998/KDD98-002.pdf)

